{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and imports"
      ],
      "metadata": {
        "id": "4Kyp0SCdGUtB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOqYT3yAF8G8",
        "outputId": "ee567b0c-bb82-4362-e9b5-21442b71b85b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LPRNet_Pytorch'...\n",
            "remote: Enumerating objects: 1172, done.\u001b[K\n",
            "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 1172 (delta 79), reused 116 (delta 73), pack-reused 1041 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1172/1172), 20.06 MiB | 14.05 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n"
          ]
        }
      ],
      "source": [
        "![ -d \"LPRNet_Pytorch\" ] && rm -rf LPRNet_Pytorch\n",
        "!git clone https://github.com/bdbux/LPRNet_Pytorch.git\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from collections import OrderedDict, defaultdict\n",
        "from typing import Union, List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tvm==1.0.0\n",
        "!python3 -m  pip install mlc-ai-cpu -f https://mlc.ai/wheels\n",
        "import tvm\n",
        "from tvm.ir.module import IRModule\n",
        "from tvm.script import tir as T, relax as R\n",
        "from tvm import relax\n",
        "import numpy as np\n",
        "\n",
        "# This is needed for deferring annotation parsing in TVMScript\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import fx\n",
        "from torch.nn import functional as F\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jt5qMfHBGQ82",
        "outputId": "ff76144e-7068-4045-9f9f-31ec61d57df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tvm==1.0.0 in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from tvm==1.0.0) (1.4.4)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from tvm==1.0.0) (0.6.2)\n",
            "Requirement already satisfied: inform in /usr/local/lib/python3.10/dist-packages (from tvm==1.0.0) (1.32)\n",
            "Requirement already satisfied: quantiphy in /usr/local/lib/python3.10/dist-packages (from tvm==1.0.0) (2.20)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.10/dist-packages (from inform->tvm==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from inform->tvm==1.0.0) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->inform->tvm==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow->inform->tvm==1.0.0) (2.9.0.20241206)\n",
            "Looking in links: https://mlc.ai/wheels\n",
            "Requirement already satisfied: mlc-ai-cpu in /usr/local/lib/python3.10/dist-packages (0.17.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (24.2.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (3.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (1.13.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (6.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtune\n",
        "!pip install torchao\n",
        "import torchtune as tt\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMXZ5HAJt4Ok",
        "outputId": "690f22b1-77e8-40bb-c7d2-8b770a6013bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtune in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from torchtune) (3.1.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.26.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.4.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from torchtune) (0.8.0)\n",
            "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.10/dist-packages (from torchtune) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtune) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtune) (4.66.6)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (from torchtune) (2.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from torchtune) (5.9.5)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from torchtune) (11.0.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (3.21.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (2.2.3)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (5.3.0)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->torchtune) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->torchtune) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (3.11.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->torchtune) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->torchtune) (4.12.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf->torchtune) (4.9.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->torchtune) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->torchtune) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->torchtune) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->torchtune) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->torchtune) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->torchtune) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.16.0)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.10/dist-packages (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def get_sparsity(tensor: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    calculate the sparsity of the given tensor\n",
        "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
        "    \"\"\"\n",
        "    return 1 - float(tensor.count_nonzero()) / tensor.numel()\n",
        "\n",
        "\n",
        "def get_model_sparsity(model: nn.Module) -> float:\n",
        "    \"\"\"\n",
        "    calculate the sparsity of the given model\n",
        "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
        "    \"\"\"\n",
        "    num_nonzeros, num_elements = 0, 0\n",
        "    for param in model.parameters():\n",
        "        num_nonzeros += param.count_nonzero()\n",
        "        num_elements += param.numel()\n",
        "    return 1 - float(num_nonzeros) / num_elements\n",
        "\n",
        "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
        "    \"\"\"\n",
        "    calculate the total number of parameters of model\n",
        "    :param count_nonzero_only: only count nonzero weights\n",
        "    \"\"\"\n",
        "    num_counted_elements = 0\n",
        "    for param in model.parameters():\n",
        "        if count_nonzero_only:\n",
        "            num_counted_elements += param.count_nonzero()\n",
        "        else:\n",
        "            num_counted_elements += param.numel()\n",
        "    return num_counted_elements\n",
        "\n",
        "\n",
        "def get_model_size(model: nn.Module, data_width=32, count_nonzero_only=False) -> int:\n",
        "    \"\"\"\n",
        "    calculate the model size in bits\n",
        "    :param data_width: #bits per element\n",
        "    :param count_nonzero_only: only count nonzero weights\n",
        "    \"\"\"\n",
        "    return get_num_parameters(model, count_nonzero_only) * data_width\n"
      ],
      "metadata": {
        "id": "xZHDE6KJnrdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Original Model Testing\n",
        "\n",
        "Note that model size is calculated similar to how it was calculated for the pruning project. Onnx was unable to export the LPRNet model into an onnx file without extensive debugging as the provided guide was not very helpful."
      ],
      "metadata": {
        "id": "Drw3wS0wT8Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os, torch\n",
        "sys.path.append('LPRNet_Pytorch')\n",
        "sys.setrecursionlimit(10000) # apparently this works?\n",
        "\n",
        "# from test_LPRNet import get_parser, get_model\n",
        "import test_LPRNet\n",
        "device = torch.device('cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Load arguments and customize if needed\n",
        "args = test_LPRNet.get_parser()\n",
        "args.pretrained_model = \"LPRNet_Pytorch/weights/Final_LPRNet_model.pth\"\n",
        "\n",
        "# Instantiate the model\n",
        "model = test_LPRNet.get_model(args)\n",
        "\n",
        "model_size = get_model_size(model, data_width=32)\n",
        "print(f\"Model size: {model_size / (1024 * 1024):.2f} MB\")\n",
        "model_sparsity = get_model_sparsity(model)\n",
        "print(f\"Model sparsity: {model_sparsity:.4f}\")\n",
        "\n",
        "print(\"Testing Original Model\")\n",
        "test_LPRNet.test(args, model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w90TGZp2Q5-v",
        "outputId": "cae8805c-f79d-4ca6-b0f6-391d2e1f2905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Loaded pretrained model successfully!\n",
            "Model size: 13.64 MB\n",
            "Model sparsity: 0.0000\n",
            "Testing Original Model\n",
            "Build successful with provided model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/LPRNet_Pytorch/test_LPRNet.py:222: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(args.pretrained_model, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.897 [897:61:42:1000]\n",
            "[Info] Individual Test Speed: 0.21846267294883728s 1/1000]\n",
            "Total time: 218.46267294883728 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RelaxIR module (MLC1)"
      ],
      "metadata": {
        "id": "2Axjx3NcEfan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Not currently working - please see error message saved below\n",
        "# and report for more details\n",
        "\"\"\"\n",
        "state_dict = torch.load(\"LPRNet_Pytorch/weights/Final_LPRNet_model.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "def map_param(param: nn.Parameter):\n",
        "    ndim = len(param.data.shape)\n",
        "    return relax.const(\n",
        "        param.data.cpu().numpy(), relax.DynTensorType(ndim, \"float32\")\n",
        "    )\n",
        "\n",
        "def fetch_attr(fx_mod, target: str):\n",
        "    target_atoms = target.split('.')\n",
        "    attr_itr = fx_mod\n",
        "    for i, atom in enumerate(target_atoms):\n",
        "        if not hasattr(attr_itr, atom):\n",
        "            raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n",
        "        attr_itr = getattr(attr_itr, atom)\n",
        "    return attr_itr\n",
        "\n",
        "# ReLU\n",
        "def map_nn_relu_op(bb, node_map, node, nn_mod):\n",
        "    A = node_map[node.args[0]]\n",
        "    return bb.emit(relax.op.nn.relu(A))\n",
        "\n",
        "# Linear\n",
        "def map_nn_linear_op(bb, node_map, node, nn_mod):\n",
        "    # print(node_map)\n",
        "    # print(node)\n",
        "    # if node.args[0] == \"view\":\n",
        "    #     return map_nn_view_op(bb, node_map, node, nn_mod)\n",
        "\n",
        "    x = node_map[node.args[0]]\n",
        "    w = map_param(state_dict[f\"{node.target}.weight\"])\n",
        "    if nn_mod.bias is not None:\n",
        "        b = map_param(nn_mod.bias)\n",
        "    return bb.emit(relax.op.linear(x, w, b))\n",
        "\n",
        "# View\n",
        "def map_nn_view_op(bb, node_map, node, nn_mod):\n",
        "    #print(node_map)\n",
        "    #print(node)\n",
        "    x = node_map[node.args[0]]\n",
        "    new_shape = tuple(node.args[1:])\n",
        "    return bb.emit(relax.op.reshape(x, new_shape))\n",
        "\n",
        "# Conv2d\n",
        "def map_nn_conv2d_op(bb, node_map, node, nn_mod):\n",
        "    x = node_map[node.args[0]]\n",
        "\n",
        "    weight = map_param(state_dict[f\"{node.target}.weight\"])\n",
        "    bias = map_param(state_dict[f\"{node.target}.bias\"]) if nn_mod.bias is not None else None\n",
        "\n",
        "    stride = nn_mod.stride\n",
        "    padding = nn_mod.padding\n",
        "    dilation = nn_mod.dilation\n",
        "    groups = nn_mod.groups\n",
        "\n",
        "    conv = bb.emit(relax.op.nn.conv2d(\n",
        "        x, weight, strides=stride, padding=padding, dilation=dilation, groups=groups\n",
        "    ))\n",
        "\n",
        "    # Apply bias if it exists\n",
        "    if bias is not None:\n",
        "        # Reshape bias to match the convolutional output\n",
        "        bias = bb.emit(relax.op.reshape(bias, (1, -1, 1, 1)))\n",
        "        conv = bb.emit(relax.op.add(conv, bias))\n",
        "\n",
        "    return conv\n",
        "\n",
        "#BatchNorm2d\n",
        "def map_nn_batchnorm2d_op(bb, node_map, node, nn_mod):\n",
        "    x = node_map[node.args[0]]\n",
        "\n",
        "    # Map parameters for BatchNorm2d: weight (gamma), bias (beta), running mean, running var\n",
        "    gamma = map_param(state_dict[f\"{node.target}.weight\"]) if nn_mod.weight is not None else None\n",
        "    beta = map_param(state_dict[f\"{node.target}.bias\"]) if nn_mod.bias is not None else None\n",
        "    running_mean = map_param(state_dict[f\"{node.target}.running_mean\"])\n",
        "    running_var = map_param(state_dict[f\"{node.target}.running_var\"])\n",
        "\n",
        "    # Extract BatchNorm2d attributesm\n",
        "    eps = nn_mod.eps\n",
        "    momentum = nn_mod.momentum\n",
        "\n",
        "    return bb.emit(relax.op.nn.batch_norm(\n",
        "        x, gamma, beta, running_mean, running_var, axis=1, epsilon=eps, momentum=momentum\n",
        "    )[0])\n",
        "\n",
        "# Max Pool\n",
        "def map_nn_maxpool3d_op(bb, node_map, node, nn_mod):\n",
        "    # print(f\"Node map: {node_map.keys()}\")\n",
        "    # print(\"NODE ARGS:\")\n",
        "    # print(node.args)\n",
        "    # Extract input tensor\n",
        "    x = node_map[node.args[0]]\n",
        "    # print(f\"X: {x}\")\n",
        "\n",
        "    kernel_size = (1, 3, 3)\n",
        "    stride = node.kwargs.get('stride')\n",
        "    if stride is None:\n",
        "        stride = kernel_size\n",
        "    if not isinstance(stride, tuple):\n",
        "        stride = (stride, stride)\n",
        "\n",
        "    padding = node.kwargs.get('padding', 0)\n",
        "    dilation = node.kwargs.get('dilation', 1)\n",
        "\n",
        "    return bb.emit(relax.op.nn.max_pool3d(\n",
        "        x, pool_size=kernel_size, strides=stride, padding=padding, dilation=dilation\n",
        "    ))\n",
        "\n",
        "# AvgPool2d\n",
        "def map_nn_avgpool2d_op(bb, node_map, node, nn_mod):\n",
        "    x = node_map[node.args[0]]\n",
        "\n",
        "    # extract kernel size, stride, and padding\n",
        "    kernel_size = nn_mod.kernel_size\n",
        "    stride = nn_mod.stride if nn_mod.stride is not None else kernel_size\n",
        "    padding = nn_mod.padding\n",
        "\n",
        "    return bb.emit(relax.op.nn.avg_pool2d(\n",
        "        x, pool_size=kernel_size, strides=stride, padding=padding\n",
        "    ))\n",
        "\n",
        "# Translate pytorch computational graph\n",
        "def from_fx(fx_mod, input_shapes, call_function_map, call_module_map):\n",
        "    input_index = 0\n",
        "    node_map = {}\n",
        "    named_modules = dict(fx_mod.named_modules())\n",
        "    nn_mod = named_modules\n",
        "\n",
        "    bb = relax.BlockBuilder()\n",
        "\n",
        "    fn_inputs = []\n",
        "    fn_output = None\n",
        "\n",
        "    with bb.function(\"main\"):\n",
        "        with bb.dataflow():\n",
        "            for node in fx_mod.graph.nodes:\n",
        "                if node.op == \"placeholder\":\n",
        "                    # create input placeholder\n",
        "                    shape = input_shapes[input_index]\n",
        "                    input_index += 1\n",
        "                    input_var = relax.Var(node.target, relax.TensorStructInfo(shape, \"float32\"))\n",
        "                    fn_inputs.append(input_var)\n",
        "                    node_map[node] = input_var\n",
        "                elif node.op == \"get_attr\":\n",
        "                    node_map[node] = map_param(fetch_attr(fx_mod, node.target))\n",
        "                elif node.op == \"call_function\":\n",
        "                    if node.target in call_function_map:\n",
        "                        node_map[node] = call_function_map[node.target](bb, node_map, node)\n",
        "                elif node.op == \"call_module\":\n",
        "                    named_module = named_modules[node.target]\n",
        "                    node_map[node] = call_module_map[type(named_module)](bb, node_map, node, named_module)\n",
        "                elif node.op == \"call_method\":\n",
        "                    # Explicitly handle view\n",
        "                    if node.target == \"view\":\n",
        "                        node_map[node] = map_nn_view_op(bb, node_map, node, fx_mod)\n",
        "                elif node.op == \"output\":\n",
        "                    output = node_map[node.args[0]]\n",
        "                    assert fn_output is None\n",
        "                    fn_output = bb.emit_output(output)\n",
        "\n",
        "        bb.emit_func_output(fn_output, fn_inputs)\n",
        "\n",
        "        print(node_map)\n",
        "        print(bb)\n",
        "\n",
        "    return bb.get()\n",
        "\n",
        "import torch.fx\n",
        "\n",
        "# use fx graph translation\n",
        "fx_model = fx.symbolic_trace(model)\n",
        "# print(fx_model.print_readable)\n",
        "\n",
        "# translate from FX representation to Relax IR module\n",
        "RelaxModule = from_fx(\n",
        "    fx_model,\n",
        "    input_shapes = [(1, 3, 94, 24)],\n",
        "    call_function_map = {\n",
        "        torch.nn.functional.relu: map_nn_relu_op,\n",
        "        torch.nn.functional.max_pool3d: map_nn_maxpool3d_op,\n",
        "        torch.Tensor.view: map_nn_view_op,\n",
        "\n",
        "    },\n",
        "    call_module_map={\n",
        "        torch.nn.Linear: map_nn_linear_op,\n",
        "        torch.nn.Conv2d: map_nn_conv2d_op,\n",
        "        torch.nn.BatchNorm2d: map_nn_batchnorm2d_op,\n",
        "        torch.nn.AvgPool2d: map_nn_avgpool2d_op,\n",
        "        torch.nn.ReLU: map_nn_relu_op,\n",
        "        torch.nn.MaxPool3d: map_nn_maxpool3d_op,\n",
        "    },\n",
        ")\n",
        "\n",
        "RelaxModule.show()\n",
        "ex = relax.vm_build.build(RelaxModule, target=\"llvm\")\n",
        "print(ex)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Wuh_p0alLNlr",
        "outputId": "3944bc21-b685-42f9-e403-2a01af4372eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nstate_dict = torch.load(\"LPRNet_Pytorch/weights/Final_LPRNet_model.pth\", map_location=torch.device(\\'cpu\\'))\\n\\ndef map_param(param: nn.Parameter):\\n    ndim = len(param.data.shape)\\n    return relax.const(\\n        param.data.cpu().numpy(), relax.DynTensorType(ndim, \"float32\")\\n    )\\n\\ndef fetch_attr(fx_mod, target: str):\\n    target_atoms = target.split(\\'.\\')\\n    attr_itr = fx_mod\\n    for i, atom in enumerate(target_atoms):\\n        if not hasattr(attr_itr, atom):\\n            raise RuntimeError(f\"Node referenced nonexistant target {\\'.\\'.join(target_atoms[:i])}\")\\n        attr_itr = getattr(attr_itr, atom)\\n    return attr_itr\\n\\n# ReLU\\ndef map_nn_relu_op(bb, node_map, node, nn_mod):\\n    A = node_map[node.args[0]]\\n    return bb.emit(relax.op.nn.relu(A))\\n\\n# Linear\\ndef map_nn_linear_op(bb, node_map, node, nn_mod):\\n    # print(node_map)\\n    # print(node)\\n    # if node.args[0] == \"view\":\\n    #     return map_nn_view_op(bb, node_map, node, nn_mod)\\n\\n    x = node_map[node.args[0]]\\n    w = map_param(state_dict[f\"{node.target}.weight\"])\\n    if nn_mod.bias is not None:\\n        b = map_param(nn_mod.bias)\\n    return bb.emit(relax.op.linear(x, w, b))\\n\\n# View\\ndef map_nn_view_op(bb, node_map, node, nn_mod):\\n    #print(node_map)\\n    #print(node)\\n    x = node_map[node.args[0]]\\n    new_shape = tuple(node.args[1:])\\n    return bb.emit(relax.op.reshape(x, new_shape))\\n\\n# Conv2d\\ndef map_nn_conv2d_op(bb, node_map, node, nn_mod):\\n    x = node_map[node.args[0]]\\n\\n    weight = map_param(state_dict[f\"{node.target}.weight\"])\\n    bias = map_param(state_dict[f\"{node.target}.bias\"]) if nn_mod.bias is not None else None\\n\\n    stride = nn_mod.stride\\n    padding = nn_mod.padding\\n    dilation = nn_mod.dilation\\n    groups = nn_mod.groups\\n\\n    conv = bb.emit(relax.op.nn.conv2d(\\n        x, weight, strides=stride, padding=padding, dilation=dilation, groups=groups\\n    ))\\n\\n    # Apply bias if it exists\\n    if bias is not None:\\n        # Reshape bias to match the convolutional output\\n        bias = bb.emit(relax.op.reshape(bias, (1, -1, 1, 1)))\\n        conv = bb.emit(relax.op.add(conv, bias))\\n\\n    return conv\\n\\n#BatchNorm2d\\ndef map_nn_batchnorm2d_op(bb, node_map, node, nn_mod):\\n    x = node_map[node.args[0]]\\n\\n    # Map parameters for BatchNorm2d: weight (gamma), bias (beta), running mean, running var\\n    gamma = map_param(state_dict[f\"{node.target}.weight\"]) if nn_mod.weight is not None else None\\n    beta = map_param(state_dict[f\"{node.target}.bias\"]) if nn_mod.bias is not None else None\\n    running_mean = map_param(state_dict[f\"{node.target}.running_mean\"])\\n    running_var = map_param(state_dict[f\"{node.target}.running_var\"])\\n\\n    # Extract BatchNorm2d attributesm\\n    eps = nn_mod.eps\\n    momentum = nn_mod.momentum\\n\\n    return bb.emit(relax.op.nn.batch_norm(\\n        x, gamma, beta, running_mean, running_var, axis=1, epsilon=eps, momentum=momentum\\n    )[0])\\n\\n# Max Pool\\ndef map_nn_maxpool3d_op(bb, node_map, node, nn_mod):\\n    # print(f\"Node map: {node_map.keys()}\")\\n    # print(\"NODE ARGS:\")\\n    # print(node.args)\\n    # Extract input tensor\\n    x = node_map[node.args[0]]\\n    # print(f\"X: {x}\")\\n\\n    kernel_size = (1, 3, 3)\\n    stride = node.kwargs.get(\\'stride\\')\\n    if stride is None:\\n        stride = kernel_size\\n    if not isinstance(stride, tuple):\\n        stride = (stride, stride)\\n\\n    padding = node.kwargs.get(\\'padding\\', 0)\\n    dilation = node.kwargs.get(\\'dilation\\', 1)\\n\\n    return bb.emit(relax.op.nn.max_pool3d(\\n        x, pool_size=kernel_size, strides=stride, padding=padding, dilation=dilation\\n    ))\\n\\n# AvgPool2d\\ndef map_nn_avgpool2d_op(bb, node_map, node, nn_mod):\\n    x = node_map[node.args[0]]\\n\\n    # extract kernel size, stride, and padding\\n    kernel_size = nn_mod.kernel_size\\n    stride = nn_mod.stride if nn_mod.stride is not None else kernel_size\\n    padding = nn_mod.padding\\n\\n    return bb.emit(relax.op.nn.avg_pool2d(\\n        x, pool_size=kernel_size, strides=stride, padding=padding\\n    ))\\n\\n# Translate pytorch computational graph\\ndef from_fx(fx_mod, input_shapes, call_function_map, call_module_map):\\n    input_index = 0\\n    node_map = {}\\n    named_modules = dict(fx_mod.named_modules())\\n    nn_mod = named_modules\\n\\n    bb = relax.BlockBuilder()\\n\\n    fn_inputs = []\\n    fn_output = None\\n\\n    with bb.function(\"main\"):\\n        with bb.dataflow():\\n            for node in fx_mod.graph.nodes:\\n                if node.op == \"placeholder\":\\n                    # create input placeholder\\n                    shape = input_shapes[input_index]\\n                    input_index += 1\\n                    input_var = relax.Var(node.target, relax.TensorStructInfo(shape, \"float32\"))\\n                    fn_inputs.append(input_var)\\n                    node_map[node] = input_var\\n                elif node.op == \"get_attr\":\\n                    node_map[node] = map_param(fetch_attr(fx_mod, node.target))\\n                elif node.op == \"call_function\":\\n                    if node.target in call_function_map:\\n                        node_map[node] = call_function_map[node.target](bb, node_map, node)\\n                elif node.op == \"call_module\":\\n                    named_module = named_modules[node.target]\\n                    node_map[node] = call_module_map[type(named_module)](bb, node_map, node, named_module)\\n                elif node.op == \"call_method\":\\n                    # Explicitly handle view\\n                    if node.target == \"view\":\\n                        node_map[node] = map_nn_view_op(bb, node_map, node, fx_mod)\\n                elif node.op == \"output\":\\n                    output = node_map[node.args[0]]\\n                    assert fn_output is None\\n                    fn_output = bb.emit_output(output)\\n\\n        bb.emit_func_output(fn_output, fn_inputs)\\n\\n        print(node_map)\\n        print(bb)\\n\\n    return bb.get()\\n\\nimport torch.fx\\n\\n# use fx graph translation\\nfx_model = fx.symbolic_trace(model)\\n# print(fx_model.print_readable)\\n\\n# translate from FX representation to Relax IR module\\nRelaxModule = from_fx(\\n    fx_model,\\n    input_shapes = [(1, 3, 94, 24)],\\n    call_function_map = {\\n        torch.nn.functional.relu: map_nn_relu_op,\\n        torch.nn.functional.max_pool3d: map_nn_maxpool3d_op,\\n        torch.Tensor.view: map_nn_view_op,\\n\\n    },\\n    call_module_map={\\n        torch.nn.Linear: map_nn_linear_op,\\n        torch.nn.Conv2d: map_nn_conv2d_op,\\n        torch.nn.BatchNorm2d: map_nn_batchnorm2d_op,\\n        torch.nn.AvgPool2d: map_nn_avgpool2d_op,\\n        torch.nn.ReLU: map_nn_relu_op,\\n        torch.nn.MaxPool3d: map_nn_maxpool3d_op,\\n    },\\n)\\n\\nRelaxModule.show()\\nex = relax.vm_build.build(RelaxModule, target=\"llvm\")\\nprint(ex)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# RelaxIR module error message\n",
        "Node map: dict_keys([x, backbone_0, backbone_1, backbone_2])\n",
        "NODE ARGS:\n",
        "(backbone_2,)\n",
        "X: lv5\n",
        "---------------------------------------------------------------------------\n",
        "TVMError                                  Traceback (most recent call last)\n",
        "<ipython-input-55-00b0aa009f31> in <cell line: 193>()\n",
        "    191\n",
        "    192 # Translate from FX representation to Relax IR module\n",
        "--> 193 RelaxModule = from_fx(\n",
        "    194     fx_model,\n",
        "    195     input_shapes = [(1, 3, 94, 24)],\n",
        "\n",
        "3 frames\n",
        "<ipython-input-55-00b0aa009f31> in from_fx(fx_mod, input_shapes, call_function_map, call_module_map)\n",
        "    166                 elif node.op == \"call_module\":\n",
        "    167                     named_module = named_modules[node.target]\n",
        "--> 168                     node_map[node] = call_module_map[type(named_module)](bb, node_map, node, named_module)\n",
        "    169                 elif node.op == \"call_method\":\n",
        "    170                     # Explicitly handle view\n",
        "\n",
        "<ipython-input-55-00b0aa009f31> in map_nn_maxpool3d_op(bb, node_map, node, nn_mod)\n",
        "    118\n",
        "    119     # Emit the Relax max pool 3d operation\n",
        "--> 120     return bb.emit(relax.op.nn.max_pool3d(\n",
        "    121         x, pool_size=kernel_size, strides=stride, padding=padding, dilation=dilation\n",
        "    122     ))\n",
        "\n",
        "/usr/local/lib/python3.10/dist-packages/tvm/relax/block_builder.py in emit(self, expr, name_hint)\n",
        "    321         \"\"\"\n",
        "    322         expr = self._normalize_python_tuple(expr)\n",
        "--> 323         return _ffi_api.BlockBuilderEmit(self, expr, name_hint)  # type: ignore\n",
        "    324\n",
        "    325     def call_te(self, func: Callable, *args: Any, **kwargs: Any) -> Expr:\n",
        "\n",
        "tvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.PackedFuncBase.__call__()\n",
        "\n",
        "tvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.FuncCall()\n",
        "\n",
        "tvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.FuncCall3()\n",
        "\n",
        "tvm/_ffi/_cython/./base.pxi in tvm._ffi._cy3.core.CHECK_CALL()\n",
        "\n",
        "/usr/local/lib/python3.10/dist-packages/tvm/_ffi/base.py in raise_last_ffi_error()\n",
        "    479     _LIB.TVMDropLastPythonError()\n",
        "    480\n",
        "--> 481     raise py_err\n",
        "    482\n",
        "    483\n",
        "\n",
        "TVMError: Traceback (most recent call last):\n",
        "  13: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::relax::Var (tvm::relax::BlockBuilder, tvm::RelayExpr, tvm::runtime::String)>::AssignTypedLambda<tvm::relax::__mk_TVM6::{lambda(tvm::relax::BlockBuilder, tvm::RelayExpr, tvm::runtime::String)#1}>(tvm::relax::__mk_TVM6::{lambda(tvm::relax::BlockBuilder, tvm::RelayExpr, tvm::runtime::String)#1}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)\n",
        "  12: tvm::relax::BlockBuilderImpl::Emit(tvm::RelayExpr, tvm::runtime::String)\n",
        "  11: tvm::relax::BlockBuilderImpl::Emit(tvm::RelayExpr, bool, tvm::runtime::String)\n",
        "  10: tvm::relax::Normalizer::Normalize(tvm::RelayExpr const&)\n",
        "  9: tvm::relax::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>::VisitExpr(tvm::RelayExpr const&)\n",
        "  8: _ZZN3tvm5relax11ExprFuncto\n",
        "  7: tvm::relax::Normalizer::VisitExpr_(tvm::relax::CallNode const*)\n",
        "  6: tvm::relax::Normalizer::InferStructInfo(tvm::relax::Call const&)\n",
        "  5: _ZN3tvm7runtime13PackedFun\n",
        "  4: tvm::runtime::TypedPackedFunc<tvm::relax::StructInfo (tvm::relax::Call const&, tvm::relax::BlockBuilder const&)>::AssignTypedLambda<tvm::relax::StructInfo (*)(tvm::relax::Call const&, tvm::relax::BlockBuilder const&)>(tvm::relax::StructInfo (*)(tvm::relax::Call const&, tvm::relax::BlockBuilder const&))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const\n",
        "  3: tvm::relax::InferStructInfoPool3D(tvm::relax::Call const&, tvm::relax::BlockBuilder const&)\n",
        "  2: tvm::relax::CheckNdimPerLayoutAndGetShape(tvm::relax::Call const&, tvm::relax::BlockBuilder const&, tvm::relax::TensorStructInfo const&, tvm::tir::Layout const&)\n",
        "  1: _ZN3tvm5relax16BlockBuilderImpl11ReportFatalERKNS_1\n",
        "  0: _ZN3tvm7runtime6deta\n",
        "  File \"/workspace/tvm/src/relax/ir/block_builder.cc\", line 158\n",
        "TVMError: In Op(relax.nn.max_pool3d), layout NCDHW requires the input to be 5-dim tensor. However, the given input has ndim 4\n",
        "```"
      ],
      "metadata": {
        "id": "VMeXXWaSiFml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Expression (MLC 2)"
      ],
      "metadata": {
        "id": "x_jhKoXgo_QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "from tvm import te\n",
        "import numpy as np\n",
        "\n",
        "def direct_conv2d(N, C, H, W, K, R, S, stride_h, stride_w, pad_h, pad_w, Input, Kernel):\n",
        "    P = (H - R + 2 * pad_h) // stride_h + 1\n",
        "    Q = (W - S + 2 * pad_w) // stride_w + 1\n",
        "    rc = te.reduce_axis((0, C), name=\"rc\")\n",
        "    ry = te.reduce_axis((0, R), name=\"ry\")\n",
        "    rx = te.reduce_axis((0, S), name=\"rx\")\n",
        "    Conv = te.compute(\n",
        "        (N, K, P, Q),\n",
        "        lambda n, k, p, q: te.sum(\n",
        "            Input[n, rc, p * stride_h + ry - pad_h, q * stride_w + rx - pad_w] *\n",
        "            Kernel[k, rc, ry, rx],\n",
        "            axis=[rc, ry, rx]\n",
        "        ),\n",
        "        name=\"Conv\"\n",
        "    )\n",
        "    return Conv, P, Q\n",
        "\n",
        "def relu(N, C, H, W, Input):\n",
        "    return te.compute((N, C, H, W),\n",
        "                      lambda n, c, h, w: te.max(Input[n, c, h, w], tvm.tir.const(0.0, \"float32\")),\n",
        "                      name=\"ReLU\")\n",
        "\n",
        "def batchnorm2d(N, C, H, W, Input, gamma, beta, mean, var, epsilon=1e-5):\n",
        "    return te.compute(\n",
        "        (N, C, H, W),\n",
        "        lambda n, c, h, w: (Input[n, c, h, w] - mean[c]) / te.sqrt(var[c] + epsilon) * gamma[c] + beta[c],\n",
        "        name=\"BatchNorm\"\n",
        "    )\n",
        "\n",
        "def maxpool2d(N, C, H, W, Input, pool_h, pool_w, stride_h, stride_w):\n",
        "    outH = (H - pool_h) // stride_h + 1\n",
        "    outW = (W - pool_w) // stride_w + 1\n",
        "    ph = te.reduce_axis((0, pool_h), \"ph\")\n",
        "    pw = te.reduce_axis((0, pool_w), \"pw\")\n",
        "    Pool = te.compute(\n",
        "        (N, C, outH, outW),\n",
        "        lambda n, c, h, w: te.max(Input[n, c, h*stride_h+ph, w*stride_w+pw], axis=[ph, pw]),\n",
        "        name=\"MaxPool2D\"\n",
        "    )\n",
        "    return Pool, outH, outW\n",
        "\n",
        "def avgpool2d(N, C, H, W, Input, pool_h, pool_w, stride_h, stride_w):\n",
        "    outH = (H - pool_h) // stride_h + 1\n",
        "    outW = (W - pool_w) // stride_w + 1\n",
        "    ph = te.reduce_axis((0, pool_h), \"ph\")\n",
        "    pw = te.reduce_axis((0, pool_w), \"pw\")\n",
        "    factor = pool_h * pool_w\n",
        "    Avg = te.compute(\n",
        "        (N, C, outH, outW),\n",
        "        lambda n, c, h, w: te.sum(Input[n, c, h*stride_h+ph, w*stride_w+pw], axis=[ph, pw]) / factor,\n",
        "        name=\"AvgPool2D\"\n",
        "    )\n",
        "    return Avg, outH, outW\n",
        "\n",
        "def small_basic_block(N, Cin, H, W, Cout, Input,\n",
        "                      W_1x1a, W_3x1, W_1x3, W_1x1b):\n",
        "\n",
        "    # 1x1 conv\n",
        "    Conv1, P1, Q1 = direct_conv2d(N, Cin, H, W, Cout//4, 1, 1, 1, 1, 0, 0, Input, W_1x1a)\n",
        "    ReLU1 = relu(N, Cout//4, P1, Q1, Conv1)\n",
        "\n",
        "    # 3x1 conv (pad=(1,0))\n",
        "    Conv2, P2, Q2 = direct_conv2d(N, Cout//4, P1, Q1, Cout//4, 3, 1, 1, 1, 1, 0, ReLU1, W_3x1)\n",
        "    ReLU2 = relu(N, Cout//4, P2, Q2, Conv2)\n",
        "\n",
        "    # 1x3 conv (pad=(0,1))\n",
        "    Conv3, P3, Q3 = direct_conv2d(N, Cout//4, P2, Q2, Cout//4, 1, 3, 1, 1, 0, 1, ReLU2, W_1x3)\n",
        "    ReLU3 = relu(N, Cout//4, P3, Q3, Conv3)\n",
        "\n",
        "    # final 1x1 conv\n",
        "    Conv4, P4, Q4 = direct_conv2d(N, Cout//4, P3, Q3, Cout, 1, 1, 1, 1, 0, 0, ReLU3, W_1x1b)\n",
        "    return Conv4, P4, Q4\n",
        "\n",
        "# Input dimensions\n",
        "N, C, H, W = 1, 3, 94, 24\n",
        "Input_tensor = te.placeholder((N, C, H, W), name=\"Input\", dtype=\"float32\")\n",
        "\n",
        "# first layer: Conv(3->64, kernel=3x3, stride=1)\n",
        "W_conv1 = te.placeholder((64, 3, 3, 3), name=\"W_conv1\", dtype=\"float32\")\n",
        "Conv1, H1, W1 = direct_conv2d(N, 3, H, W, 64, 3, 3, 1, 1, 0, 0, Input_tensor, W_conv1)\n",
        "\n",
        "# BN + ReLU on Conv1\n",
        "gamma1 = te.placeholder((64,), name=\"gamma1\")\n",
        "beta1  = te.placeholder((64,), name=\"beta1\")\n",
        "mean1  = te.placeholder((64,), name=\"mean1\")\n",
        "var1   = te.placeholder((64,), name=\"var1\")\n",
        "\n",
        "BN1 = batchnorm2d(N, 64, H1, W1, Conv1, gamma1, beta1, mean1, var1)\n",
        "ReLU1 = relu(N, 64, H1, W1, BN1)\n",
        "\n",
        "# MaxPool2d(3x3,stride=1x1)\n",
        "Pool1, H2, W2 = maxpool2d(N, 64, H1, W1, ReLU1, 3, 3, 1, 1)\n",
        "\n",
        "# small_basic_block(ch_in=64, ch_out=128)\n",
        "W_1x1a_128 = te.placeholder((128//4, 64, 1, 1), name=\"W_1x1a_128\")   # (32,64,1,1)\n",
        "W_3x1_128  = te.placeholder((128//4, 128//4, 3, 1), name=\"W_3x1_128\") # (32,32,3,1)\n",
        "W_1x3_128  = te.placeholder((128//4, 128//4, 1, 3), name=\"W_1x3_128\") # (32,32,1,3)\n",
        "W_1x1b_128 = te.placeholder((128, 128//4, 1, 1), name=\"W_1x1b_128\")   # (128,32,1,1)\n",
        "\n",
        "Block1, H3, W3 = small_basic_block(N, 64, H2, W2, 128,\n",
        "                                   Pool1, W_1x1a_128, W_3x1_128, W_1x3_128, W_1x1b_128)\n",
        "\n",
        "# BN + ReLU for Block1 output\n",
        "gamma2 = te.placeholder((128,), name=\"gamma2\")\n",
        "beta2  = te.placeholder((128,), name=\"beta2\")\n",
        "mean2  = te.placeholder((128,), name=\"mean2\")\n",
        "var2   = te.placeholder((128,), name=\"var2\")\n",
        "\n",
        "BN2 = batchnorm2d(N, 128, H3, W3, Block1, gamma2, beta2, mean2, var2)\n",
        "ReLU2 = relu(N, 128, H3, W3, BN2)\n",
        "\n",
        "# Next maxpool2d(3x3, stride=2x2)\n",
        "Pool2, H4, W4 = maxpool2d(N, 128, H3, W3, ReLU2, 3, 3, 2, 2)\n",
        "\n",
        "s = te.create_schedule([Pool2.op])\n",
        "\n",
        "te_model = tvm.build(s, [Input_tensor, W_conv1, gamma1, beta1, mean1, var1,\n",
        "                   W_1x1a_128, W_3x1_128, W_1x3_128, W_1x1b_128,\n",
        "                   gamma2, beta2, mean2, var2, Pool2],\n",
        "              target=\"llvm\", name=\"lprnet\")\n",
        "\n",
        "print(te_model)\n",
        "\n",
        "ctx = tvm.cpu(0)\n",
        "\n",
        "# create fake data just to try and test\n",
        "input_data = np.random.rand(N, C, H, W).astype(\"float32\")\n",
        "W_conv1_data = np.random.rand(64,3,3,3).astype(\"float32\")\n",
        "gamma1_data = np.random.rand(64).astype(\"float32\")\n",
        "beta1_data = np.random.rand(64).astype(\"float32\")\n",
        "mean1_data = np.random.rand(64).astype(\"float32\")\n",
        "var1_data = np.random.rand(64).astype(\"float32\")\n",
        "\n",
        "W_1x1a_128_data = np.random.rand(32,64,1,1).astype(\"float32\")\n",
        "W_3x1_128_data = np.random.rand(32,32,3,1).astype(\"float32\")\n",
        "W_1x3_128_data = np.random.rand(32,32,1,3).astype(\"float32\")\n",
        "W_1x1b_128_data = np.random.rand(128,32,1,1).astype(\"float32\")\n",
        "\n",
        "gamma2_data = np.random.rand(128).astype(\"float32\")\n",
        "beta2_data = np.random.rand(128).astype(\"float32\")\n",
        "mean2_data = np.random.rand(128).astype(\"float32\")\n",
        "var2_data = np.random.rand(128).astype(\"float32\")\n",
        "\n",
        "input_tvm = tvm.nd.array(input_data, device=ctx)\n",
        "W_conv1_tvm = tvm.nd.array(W_conv1_data, device=ctx)\n",
        "gamma1_tvm = tvm.nd.array(gamma1_data, device=ctx)\n",
        "beta1_tvm = tvm.nd.array(beta1_data, device=ctx)\n",
        "mean1_tvm = tvm.nd.array(mean1_data, device=ctx)\n",
        "var1_tvm = tvm.nd.array(var1_data, device=ctx)\n",
        "\n",
        "W_1x1a_128_tvm = tvm.nd.array(W_1x1a_128_data, device=ctx)\n",
        "W_3x1_128_tvm = tvm.nd.array(W_3x1_128_data, device=ctx)\n",
        "W_1x3_128_tvm = tvm.nd.array(W_1x3_128_data, device=ctx)\n",
        "W_1x1b_128_tvm = tvm.nd.array(W_1x1b_128_data, device=ctx)\n",
        "\n",
        "gamma2_tvm = tvm.nd.array(gamma2_data, device=ctx)\n",
        "beta2_tvm = tvm.nd.array(beta2_data, device=ctx)\n",
        "mean2_tvm = tvm.nd.array(mean2_data, device=ctx)\n",
        "var2_tvm = tvm.nd.array(var2_data, device=ctx)\n",
        "\n",
        "print(\"Pool2 shape:\", (N, 128, H4, W4))\n",
        "out_tvm = tvm.nd.empty((N, 128, H4, W4), dtype=\"float32\", device=ctx)\n",
        "\n",
        "te_model(\n",
        "    input_tvm,\n",
        "    W_conv1_tvm, gamma1_tvm, beta1_tvm, mean1_tvm, var1_tvm,\n",
        "    W_1x1a_128_tvm, W_3x1_128_tvm, W_1x3_128_tvm, W_1x1b_128_tvm,\n",
        "    gamma2_tvm, beta2_tvm, mean2_tvm, var2_tvm,\n",
        "    out_tvm\n",
        ")\n",
        "print(\"Output shape:\", out_tvm.shape)\n",
        "print(\"Output sample:\", out_tvm.asnumpy()[0,0,0:5,0:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSdlSNmXCAxz",
        "outputId": "55912c44-a920-4686-aa92-15e23c05fd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module(llvm, 5c0a52e3ccc8)\n",
            "Pool2 shape: (1, 128, 44, 9)\n",
            "Output shape: (1, 128, 44, 9)\n",
            "Output sample: [[3.2731887e+18 9.3689930e+06 9.3689930e+06 9.6803650e+06 9.6803650e+06]\n",
            " [9.8756460e+06 9.4522370e+06 9.2046290e+06 9.6803650e+06 9.6803650e+06]\n",
            " [9.8756460e+06 9.4832160e+06 8.9317540e+06 9.4175800e+06 9.4783460e+06]\n",
            " [9.4166190e+06 9.3443450e+06 8.9317540e+06 9.1230680e+06 9.6107960e+06]\n",
            " [9.1347880e+06 8.9859090e+06 8.7664820e+06 8.9080760e+06 9.4311190e+06]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Grained Pruning (MO1)"
      ],
      "metadata": {
        "id": "Ul6ts-RoeTV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_grained_prune(tensor: torch.Tensor, sparsity : float) -> torch.Tensor:\n",
        "    sparsity = min(max(0.0, sparsity), 1.0)\n",
        "    if sparsity == 1.0:\n",
        "        tensor.zero_()\n",
        "        return torch.zeros_like(tensor)\n",
        "    elif sparsity == 0.0:\n",
        "        return torch.ones_like(tensor)\n",
        "\n",
        "    num_elements = tensor.numel()\n",
        "\n",
        "    num_zeros = round(num_elements * sparsity)\n",
        "    importance = tensor.abs()\n",
        "    threshold = importance.view(-1).kthvalue(num_zeros).values\n",
        "    mask = torch.gt(importance, threshold)\n",
        "    tensor.mul_(mask)\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "_01KlIMSeXoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FineGrainedPruner:\n",
        "    def __init__(self, model, sparsity_dict):\n",
        "        self.masks = FineGrainedPruner.prune(model, sparsity_dict)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply(self, model):\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in self.masks:\n",
        "                param *= self.masks[name]\n",
        "\n",
        "    @staticmethod\n",
        "    @torch.no_grad()\n",
        "    def prune(model, sparsity_dict):\n",
        "        masks = dict()\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.dim() > 1: # we only prune conv and fc weights\n",
        "                if isinstance(sparsity_dict, dict):\n",
        "                    masks[name] = fine_grained_prune(param, sparsity_dict[name])\n",
        "                else:\n",
        "                    assert(sparsity_dict < 1 and sparsity_dict >= 0)\n",
        "                    if sparsity_dict > 0:\n",
        "                        masks[name] = fine_grained_prune(param, sparsity_dict)\n",
        "        return masks"
      ],
      "metadata": {
        "id": "foh_545KepOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparsity = 0.1\n",
        "sparse_model = copy.deepcopy(model)\n",
        "\n",
        "while sparsity < 1:\n",
        "\n",
        "    pruner = FineGrainedPruner(sparse_model, sparsity)\n",
        "    pruner.apply(sparse_model)\n",
        "    print(f\"Pruned model with sparsity: {sparsity}\")\n",
        "    sparse_model_size = get_model_size(sparse_model, data_width=32, count_nonzero_only=True)\n",
        "\n",
        "    print(f\"Model size: {sparse_model_size / (1024 * 1024):.2f} MB\")\n",
        "    test_LPRNet.test(args, sparse_model)\n",
        "\n",
        "    print(\"---------------\")\n",
        "    print()\n",
        "\n",
        "    sparsity += 0.1"
      ],
      "metadata": {
        "id": "LxyBVwqRfCik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "#RAW DATA WITH 0.1 INCREMENTS\n",
        "Pruned model with sparsity: 0.1\n",
        "Model size: 12.29 MB\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.898 [898:62:40:1000]\n",
        "[Info] Individual Test Speed: 0.17621557784080505s 1/1000]\n",
        "Total time: 176.21557784080505 seconds\n",
        "---------------\n",
        "\n",
        "Pruned model with sparsity: 0.2\n",
        "Model size: 10.93 MB\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.898 [898:64:38:1000]\n",
        "[Info] Individual Test Speed: 0.17063515448570252s 1/1000]\n",
        "Total time: 170.63515448570251 seconds\n",
        "---------------\n",
        "\n",
        "Pruned model with sparsity: 0.30000000000000004\n",
        "Model size: 9.58 MB\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.884 [884:71:45:1000]\n",
        "[Info] Individual Test Speed: 0.15751717019081116s 1/1000]\n",
        "Total time: 157.51717019081116 seconds\n",
        "---------------\n",
        "\n",
        "Pruned model with sparsity: 0.4\n",
        "Model size: 8.23 MB\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.876 [876:76:48:1000]\n",
        "[Info] Individual Test Speed: 0.1495432131290436s 1/1000]\n",
        "Total time: 149.54321312904358 seconds\n",
        "---------------\n",
        "\n",
        "Pruned model with sparsity: 0.5\n",
        "Model size: 6.88 MB\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.822 [822:102:76:1000]\n",
        "[Info] Individual Test Speed: 0.1377841682434082s 1/1000]\n",
        "Total time: 137.7841682434082 seconds\n",
        "---------------\n",
        "\n",
        "Pruned model with sparsity: 0.6\n",
        "Model size: 5.52 MB\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.677 [677:156:167:1000]\n",
        "[Info] Individual Test Speed: 0.13184543347358704s 1/1000]\n",
        "Total time: 131.84543347358704 seconds\n",
        "---------------\n",
        "\n",
        "Pruned model with sparsity: 0.7\n",
        "Model size: 4.17 MB\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.017 [17:869:114:1000]\n",
        "[Info] Individual Test Speed: 0.11505629849433899s 1/1000]\n",
        "Total time: 115.05629849433899 seconds\n",
        "---------------\n",
        "\n",
        "Pruned model with sparsity: 0.7999999999999999\n",
        "Model size: 2.82 MB\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.0 [0:944:56:1000]\n",
        "[Info] Individual Test Speed: 0.07019736027717591s 1/1000]\n",
        "Total time: 70.1973602771759 seconds\n",
        "---------------\n",
        "\n",
        "Pruned model with sparsity: 0.8999999999999999\n",
        "Model size: 1.46 MB\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.0 [0:924:76:1000]\n",
        "[Info] Individual Test Speed: 0.029899073123931884s 1/1000]\n",
        "Total time: 29.899073123931885 seconds\n",
        "---------------\n",
        "\n",
        "Pruned model with sparsity: 0.9999999999999999\n",
        "Model size: 0.11 MB\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.0 [0:1000:0:1000]\n",
        "[Info] Individual Test Speed: 0.028096100807189942s 1/1000]\n",
        "Total time: 28.09610080718994 seconds\n",
        "---------------\n",
        "```"
      ],
      "metadata": {
        "id": "9U_gE7IZrXCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fusion and Static Quantization (MO2)\n",
        "\n",
        "Quantization done following this guide: https://pytorch.org/docs/stable/quantization.html\n"
      ],
      "metadata": {
        "id": "H-YPsxdit0m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization\n",
        "from torch.quantization import fuse_modules, get_default_qconfig, prepare, convert\n",
        "import copy\n",
        "\n",
        "qmodel = copy.deepcopy(model).to('cpu').eval()\n",
        "\n",
        "# Set quantization engine\n",
        "torch.backends.quantized.engine = 'x86'\n",
        "\n",
        "fusion_list = [\n",
        "    # Initial Conv-BN-ReLU\n",
        "    [\"backbone.0\", \"backbone.1\", \"backbone.2\"],\n",
        "\n",
        "    # small_basic_block at backbone.4\n",
        "    # Conv->ReLU three times, then a final Conv\n",
        "    [\"backbone.4.block.0\", \"backbone.4.block.1\"],\n",
        "    [\"backbone.4.block.2\", \"backbone.4.block.3\"],\n",
        "    [\"backbone.4.block.4\", \"backbone.4.block.5\"],\n",
        "    # Fuse last Conv of block.4 with following BN and ReLU\n",
        "    [\"backbone.4.block.6\", \"backbone.5\", \"backbone.6\"],\n",
        "\n",
        "    # small_basic_block at backbone.8\n",
        "    [\"backbone.8.block.0\", \"backbone.8.block.1\"],\n",
        "    [\"backbone.8.block.2\", \"backbone.8.block.3\"],\n",
        "    [\"backbone.8.block.4\", \"backbone.8.block.5\"],\n",
        "    # Fuse last Conv of block.8 with following BN and ReLU\n",
        "    [\"backbone.8.block.6\", \"backbone.9\", \"backbone.10\"],\n",
        "\n",
        "    # small_basic_block at backbone.11\n",
        "    [\"backbone.11.block.0\", \"backbone.11.block.1\"],\n",
        "    [\"backbone.11.block.2\", \"backbone.11.block.3\"],\n",
        "    [\"backbone.11.block.4\", \"backbone.11.block.5\"],\n",
        "    # Fuse last Conv of block.11 with following BN and ReLU\n",
        "    [\"backbone.11.block.6\", \"backbone.12\", \"backbone.13\"],\n",
        "\n",
        "    # Fuse Conv-BN-ReLU near theend\n",
        "    [\"backbone.16\", \"backbone.17\", \"backbone.18\"],\n",
        "    [\"backbone.20\", \"backbone.21\", \"backbone.22\"],\n",
        "]\n",
        "\n",
        "\n",
        "# Make a copy of the model to fuse\n",
        "model_fused = fuse_modules(qmodel, fusion_list, inplace=False)\n",
        "\n",
        "model_fused.qconfig = get_default_qconfig('x86')\n",
        "\n",
        "# Calibrate the model?\n",
        "print(\"Testing fused model\")\n",
        "\n",
        "test_LPRNet.test(args, model_fused)\n",
        "\n",
        "fused_model_size = get_model_size(model_fused, data_width=32)\n",
        "print(f\"Fused Model size: {fused_model_size / (1024 * 1024):.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yuft7AvhoD48",
        "outputId": "8eebcfcd-b3a9-495d-a5a4-49874764b25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing fused model\n",
            "Build successful with provided model!\n",
            "[Info] Test Accuracy: 0.897 [897:61:42:1000]\n",
            "[Info] Individual Test Speed: 0.03599678826332092s 1/1000]\n",
            "Total time: 35.99678826332092 seconds\n",
            "Fused Model size: 13.58 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Raw fused model results\n",
        "Testing fused model\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.899 [899:58:43:1000]\n",
        "[Info] Individual Test Speed: 0.03917880201339722s 1/1000]\n",
        "Total time: 39.17880201339722 seconds\n",
        "Model size: 13.58 Mb\n",
        "```"
      ],
      "metadata": {
        "id": "jcbvOsNiUJL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine fusion and unstructured pruning\n",
        "combined_model = copy.deepcopy(model_fused)\n",
        "pruner = FineGrainedPruner(combined_model, 0.4)\n",
        "pruner.apply(combined_model)\n",
        "\n",
        "combined_model_size = get_model_size(combined_model, data_width=32, count_nonzero_only=True)\n",
        "\n",
        "print(f\"Combined Model size: {combined_model_size / (1024 * 1024):.2f} MB\")\n",
        "test_LPRNet.test(args, combined_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj9ut-UY_WHc",
        "outputId": "157bbf93-e416-4f88-955f-013c25f66398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Model size: 6.77 MB\n",
            "Build successful with provided model!\n",
            "[Info] Test Accuracy: 0.873 [873:80:47:1000]\n",
            "[Info] Individual Test Speed: 0.034136982679367066s 1/1000]\n",
            "Total time: 34.136982679367065 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Combined model data\n",
        "Combined Model size: 6.77 MB\n",
        "Build successful with provided model!\n",
        "[Info] Test Accuracy: 0.872 [872:76:52:1000]\n",
        "[Info] Individual Test Speed: 0.033661202907562256s 1/1000]\n",
        "Total time: 33.661202907562256 seconds\n",
        "```"
      ],
      "metadata": {
        "id": "mXW2jox3Cp8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Static Quantization - Currently throws a runtime error\n",
        "# Remove triple quotes for testing\n",
        "\"\"\"\n",
        "# Prepare the model for static quantization\n",
        "prepared_model = prepare(model_fused)\n",
        "\n",
        "# convert the model to a quantized version\n",
        "quantized_model = convert(model_fused, inplace=False).to('cpu')\n",
        "prepared_model.eval()\n",
        "N, C, H, W = 100, 3, 94, 24 # should batch size be 100 or 1?\n",
        "input_data = torch.randn(N, C, H, W)\n",
        "prepared_model(input_data)\n",
        "\n",
        "quantized_model = convert(prepared_model)\n",
        "\n",
        "# print(quantized_model)\n",
        "\n",
        "print(\"Testing quantized model\")\n",
        "\n",
        "test_LPRNet.test(args, quantized_model)\n",
        "\n",
        "quantized_model_size = get_model_size(quantized_model, data_width=8)\n",
        "print(f\"Model size: {quantized_model_size / (1024 * 1024):.2f} MB\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "bXF36CP1-NhB",
        "outputId": "c9c4f9b1-5f92-428e-f890-3cff2b8ef487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Prepare the model for static quantization\\nprepared_model = prepare(model_fused)\\n\\n# convert the model to a quantized version\\nquantized_model = convert(model_fused, inplace=False).to(\\'cpu\\')\\nprepared_model.eval()\\nN, C, H, W = 100, 3, 94, 24 # should batch size be 100 or 1?\\ninput_data = torch.randn(N, C, H, W)\\nprepared_model(input_data)\\n\\nquantized_model = convert(prepared_model)\\n\\n# print(quantized_model)\\n\\nprint(\"Testing quantized model\")\\n\\ntest_LPRNet.test(args, quantized_model)\\n\\nquantized_model_size = get_model_size(quantized_model, data_width=8)\\nprint(f\"Model size: {quantized_model_size / (1024 * 1024):.2f} MB\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Quantized model Error Message\n",
        "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
        "  warnings.warn(\n",
        "---------------------------------------------------------------------------\n",
        "RuntimeError                              Traceback (most recent call last)\n",
        "<ipython-input-9-528930bb2c49> in <cell line: 9>()\n",
        "      7 N, C, H, W = 100, 3, 94, 24\n",
        "      8 input_data = torch.randn(N, C, H, W)\n",
        "----> 9 prepared_model(input_data)\n",
        "     10\n",
        "     11 quantized_model = convert(prepared_model)\n",
        "\n",
        "5 frames\n",
        "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/pooling.py in forward(self, input)\n",
        "    754\n",
        "    755     def forward(self, input: Tensor) -> Tensor:\n",
        "--> 756         return F.avg_pool2d(\n",
        "    757             input,\n",
        "    758             self.kernel_size,\n",
        "\n",
        "RuntimeError: Given input size: (256x88x9). Calculated output size: (256x22x0). Output size is too small\n",
        "```"
      ],
      "metadata": {
        "id": "9wSSMU4stKbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Previous attempt to do dynamic quantization\n",
        "# Kept just for the record, but no improvements over original model\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.quantization\n",
        "torch.backends.quantized.engine = 'fbgemm'\n",
        "\n",
        "qmodel = copy.deepcopy(model).eval()\n",
        "\n",
        "for name, module in qmodel.named_modules():\n",
        "    print(name, module)\n",
        "\n",
        "def quantize_model(model):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.quantization.convert(module, inplace=True)\n",
        "    return torch.quantization.quantize_dynamic(\n",
        "        model, {torch.nn.Conv2d, torch.nn.Linear}, dtype=torch.qint8\n",
        "    )\n",
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    qmodel, {torch.nn.Conv2d}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "quantized_model = quantize_model(qmodel)\n",
        "\n",
        "print(quantized_model)\n",
        "\n",
        "print(\"Testing quantized model\")\n",
        "test_LPRNet.test(args, quantized_model)\n",
        "\n",
        "quantized_model_size = get_model_size(quantized_model, data_width=8)\n",
        "print(f\"Model size: {quantized_model_size / (1024 * 1024):.2f} MB\")\n",
        "\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "wLDC8ZlAaHFh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3170eb0d-d64d-43d0-da9c-ce9f601f894c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport torch\\nimport torch.quantization\\ntorch.backends.quantized.engine = \\'fbgemm\\'\\n\\nqmodel = copy.deepcopy(model).eval()\\n\\nfor name, module in qmodel.named_modules():\\n    print(name, module)\\n\\ndef quantize_model(model):\\n    for name, module in model.named_modules():\\n        if isinstance(module, torch.nn.Conv2d):\\n            torch.quantization.convert(module, inplace=True)\\n    return torch.quantization.quantize_dynamic(\\n        model, {torch.nn.Conv2d, torch.nn.Linear}, dtype=torch.qint8\\n    )\\n\\nquantized_model = torch.quantization.quantize_dynamic(\\n    qmodel, {torch.nn.Conv2d}, dtype=torch.qint8\\n)\\n\\nquantized_model = quantize_model(qmodel)\\n\\nprint(quantized_model)\\n\\nprint(\"Testing quantized model\")\\ntest_LPRNet.test(args, quantized_model)\\n\\nquantized_model_size = get_model_size(quantized_model, data_width=8)\\nprint(f\"Model size: {quantized_model_size / (1024 * 1024):.2f} MB\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original testing script\n"
      ],
      "metadata": {
        "id": "PD0UHB78VTvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 LPRNet_Pytorch/test_LPRNet.py"
      ],
      "metadata": {
        "id": "KNwKsd-0MsuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Raw data\n",
        "\n",
        "Note: All testing is done using CPU rather than cuda, as some optimized models were having issues.\n",
        "\n",
        "Original Model:\n",
        "- Model size: 13.64 MB\n",
        "- Model sparsity: 0.0000\n",
        "- Build successful with provided model!\n",
        "- [Info] Test Accuracy: 0.897\n",
        "- [Info] Individual Test Speed: 0.2103862180709839s 1/1000]\n",
        "- Total time: 210.3862180709839 seconds\n",
        "\n",
        "Fused Model:\n",
        "- [Info] Test Accuracy: 0.899 [899:59:42:1000]\n",
        "- [Info] Individual Test Speed: 0.05818154287338257s 1/1000]\n",
        "- Total time: 58.18154287338257 seconds\n",
        "\n"
      ],
      "metadata": {
        "id": "irGZpOkcP5rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structure of LPRNet used to determine fusion operations\n",
        "```\n",
        "LPRNet(\n",
        "  (backbone): Sequential(\n",
        "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
        "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (2): ReLU()\n",
        "    (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n",
        "    (4): small_basic_block(\n",
        "      (block): Sequential(\n",
        "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
        "        (1): ReLU()\n",
        "        (2): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
        "        (3): ReLU()\n",
        "        (4): Conv2d(32, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
        "        (5): ReLU()\n",
        "        (6): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "      )\n",
        "    )\n",
        "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (6): ReLU()\n",
        "    (7): MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (8): small_basic_block(\n",
        "      (block): Sequential(\n",
        "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        (1): ReLU()\n",
        "        (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
        "        (3): ReLU()\n",
        "        (4): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
        "        (5): ReLU()\n",
        "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "      )\n",
        "    )\n",
        "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (10): ReLU()\n",
        "    (11): small_basic_block(\n",
        "      (block): Sequential(\n",
        "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        (1): ReLU()\n",
        "        (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
        "        (3): ReLU()\n",
        "        (4): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
        "        (5): ReLU()\n",
        "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "      )\n",
        "    )\n",
        "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (13): ReLU()\n",
        "    (14): MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
        "    (15): Dropout(p=0, inplace=False)\n",
        "    (16): Conv2d(64, 256, kernel_size=(1, 4), stride=(1, 1))\n",
        "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (18): ReLU()\n",
        "    (19): Dropout(p=0, inplace=False)\n",
        "    (20): Conv2d(256, 68, kernel_size=(13, 1), stride=(1, 1))\n",
        "    (21): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    (22): ReLU()\n",
        "  )\n",
        "  (container): Sequential(\n",
        "    (0): Conv2d(516, 68, kernel_size=(1, 1), stride=(1, 1))\n",
        "  )\n",
        ")\n",
        "```\n",
        "\n",
        "At the top level, we have Conv2d + BatchNorm + ReLU pattern at (0, 1, 2), (16, 17, 18), finally (20, 21, 22). Inside the small basic blocks we have a Conv2d + ReLU pair that we can also fuse together."
      ],
      "metadata": {
        "id": "fUqkJNiTuZ8t"
      }
    }
  ]
}